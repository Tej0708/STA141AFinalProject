---
title: "A Neural Predictor: Predictive modeling on feedback types for mice stimuli"
author: "Tej Gaonkar 920711851"
date: "March 18 2024"
output: html_document
---

# Abstract:

In this paper, the best predictor for feedback type given neural activities. This data was collected in a 2019 experiment by Steinmetz et al. where they were trying to track the neural activities of mice when exposed to visual stimuli. The dataset that I used contained 18 sessions and 4 different mice and their neural data + their response type. At first, an Exploratory Data Analysis was conducted to see if there was any relationships that could be gathered by exploring certain characteristics in the data like the meta-data, the specific neuron activation in trials, the average neuron/spike activation over a whole session, trends of success between mice, and dimensionality reduction plots with corresponding clustering analysis. After that, a detailed data integration method was shown to average out the `spks` count to form a dataframe that could be used for predictive modeling. Immediately after data integration, 4 models were chosen to be evaluated: Logistic Regression, Random Forest Classification, XGBoost, and SVM. XGBoost was then chosen to be evaluated on the testing data as it performed the best. Finally some additional corollaries and data questions were discussed pertaining to model selection and data integrity.

***

# Introduction:

This project will be analyzing the neural activity in mice and their response to certain stimuli. The data is a subset of the data collected by Steinmetz et al. (2019). For some background, The experiments were performed on a total of 10 mice over 39 distinct sessions. However, in this subset of data, we will only be looking at sessions 1 through 18 and at 4 out of the 10 mice tested. Each session consisted of hundreds of trials, where certain stimuli were presented to the mice. These stimuli had different contrast levels that measured from 0 - 1 where 0 meant no stimulus. The mice then made a decision by turning a wheel and if the decision was correct they would receive a reward (or a penalty if their decision was wrong). Here is a brief outline of how the feedback was given:

- When left contrast > right contrast, success (1) if turning the wheel to the right and failure (-1) otherwise.  
- When right contrast > left contrast, success (1) if turning the wheel to the left and failure (-1) otherwise.  
- When both left and right contrasts are zero, success (1) if holding the wheel still and failure (-1) otherwise. 
- When left and right contrasts are equal but non-zero, left or right will be randomly chosen (50%) as the correct choice. 

The data that was collected through these experiments were spike trains, which are neuron firings at different time intervals, from the mice's visual cortex.

Throughout this report, I will explore this data and try to use a predictive model technique to predict the feedback type based on the features including the neuron spikes and the contrast levels/difference. The structure of the report goes as follows:

### Part 1 Exploratory Data Analysis:

In this section, I will be exploring some of the data and trying to see if I can point out any trends just based on some plots. In particular I will start by looking at the data structure (i.e the number of trials, number of brain areas, etc.). I will then transition into looking at the specific neuron activity in each trial of a specific session, just to see if there are any trends with neuron activity over time. I will then look at the average spike counts over an entire session. After that we will examine if there are any trends with success rate and different mice. We will then try some dimensionality reduction techniques to visualize the data better and some clustering to see if we can gain more insights.

### Part 2: Data Integration:

In this section, I will be explaining how I was able to properly integrate and mold my data so that we can then feed it into a multitude of prediction models. I will also explain how I was able to pair down the spks column in the data to make a tibble to feed into the model.

### Part 3: Predictive Modeling:

In this section, I will be talking about how I split the data for model training and testing and the different models I used with their accuracy and other performance metrics

### Part 4: Discussion:

In the last section, I will be going over my results and reflecting on the project. I will be discussing the results and the final model as well as some additional corollaries and other tangents that can be explored in the future in relation to this project and achieving a better model.


***


# Exploratory Data Analysis

A total of 18 RDS files are provided that contain the records from 18 sessions. In each RDS file, you can find the name of mouse from `mouse_name` and date of the experiment from `date_exp`. 


```{r include=FALSE, eval=TRUE}
library(tidyverse)
session=list()
for(i in 1:18){
  file_path <- paste('/Users/tejgaonkar/Downloads/sessions/session', i, '.rds', sep='')
  session[[i]] <- readRDS(file_path)
  print(session[[i]]$mouse_name)
  print(session[[i]]$date_exp)
}
```


```{r include=FALSE}
library(tidyverse)
# Create data frame across sessions
n.session=length(session) 
meta <- tibble(
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  num_brain_area = rep(0,n.session),
  num_neurons = rep(0,n.session),
  num_trials = rep(0,n.session),
  success_rate = rep(0,n.session)
)
# Store data into the data frame
for(i in 1:n.session){
  tmp = session[[i]];
  meta[i,1]=tmp$mouse_name;
  meta[i,2]=tmp$date_exp;
  meta[i,3]=length(unique(tmp$brain_area));
  meta[i,4]=dim(tmp$spks[[1]])[1];
  meta[i,5]=length(tmp$feedback_type);
  meta[i,6]=mean(tmp$feedback_type+1)/2;
}
head(meta)
```
```{r echo=FALSE}
i.s=5 # indicator for this session

i.t=9 # indicator for this trial 

spk.trial = session[[i.s]]$spks[[i.t]]
area=session[[i.s]]$brain_area

spk.count=apply(spk.trial,1,sum)

spk.average.tapply=tapply(spk.count, area, mean)


# dplyr: 
# To use dplyr you need to create a data frame
tmp <- data.frame(
  area = area,
  spikes = spk.count
)
# Calculate the average by group using dplyr
spk.average.dplyr =tmp %>%
  group_by(area) %>%
  summarize(mean= mean(spikes))
```

```{r include=FALSE}
average_spike_area<-function(i.t,this_session){
  spk.trial = this_session$spks[[i.t]]
  area= this_session$brain_area
  spk.count=apply(spk.trial,1,sum)
  spk.average.tapply=tapply(spk.count, area, mean)
  return(spk.average.tapply)
  }

# Test the function
average_spike_area(1,this_session = session[[i.s]])
```
```{r echo=FALSE}
n.trial=length(session[[i.s]]$feedback_type)
n.area=length(unique(session[[i.s]]$brain_area ))
# Alternatively, you can extract these information in the meta that we created before.

# We will create a data frame that contain the average spike counts for each area, feedback type,  the two contrasts, and the trial id

trial.summary =matrix(nrow=n.trial,ncol= n.area+1+2+1)
for(i.t in 1:n.trial){
  trial.summary[i.t,]=c(average_spike_area(i.t,this_session = session[[i.s]]),
                          session[[i.s]]$feedback_type[i.t],
                        session[[i.s]]$contrast_left[i.t],
                        session[[i.s]]$contrast_right[i.s],
                        i.t)
}

colnames(trial.summary)=c(names(average_spike_area(i.t,this_session = session[[i.s]])), 'feedback', 'left contr.','right contr.','id' )

# Turning it into a data frame
trial.summary <- as_tibble(trial.summary)
```

```{r include=FALSE}
head(trial.summary)
```

```{r,echo=FALSE}
binename <- paste0("bin", as.character(1:40))

get_trial_functional_data <- function(session_id, trial_id){
  spikes <- session[[session_id]]$spks[[trial_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trial_bin_average <- matrix(colMeans(spikes), nrow = 1)
  colnames(trial_bin_average) <- binename
  trial_tibble  = as_tibble(trial_bin_average)%>% add_column("trial_id" = trial_id) %>% add_column("contrast_left"= session[[session_id]]$contrast_left[trial_id]) %>% add_column("contrast_right"= session[[session_id]]$contrast_right[trial_id]) %>% add_column("feedback_type"= session[[session_id]]$feedback_type[trial_id])
  
  trial_tibble
}
get_session_functional_data <- function(session_id){
  n_trial <- length(session[[session_id]]$spks)
  trial_list <- list()
  for (trial_id in 1:n_trial){
    trial_tibble <- get_trial_functional_data(session_id,trial_id)
    trial_list[[trial_id]] <- trial_tibble
  }
  session_tibble <- as_tibble(do.call(rbind, trial_list))
  session_tibble <- session_tibble %>% add_column("mouse_name" = session[[session_id]]$mouse_name) %>% add_column("date_exp" = session[[session_id]]$date_exp) %>% add_column("session_id" = session_id) 
  session_tibble
}

```

```{r, include= FALSE}
session_list = list()
for (session_id in 1: 18){
  session_list[[session_id]] <- get_session_functional_data(session_id)
}
full_functional_tibble <- as_tibble(do.call(rbind, session_list))
full_functional_tibble$session_id <- as.factor(full_functional_tibble$session_id )
full_functional_tibble$contrast_diff <- abs(full_functional_tibble$contrast_left-full_functional_tibble$contrast_right)

full_functional_tibble$success <- full_functional_tibble$feedback_type == 1
full_functional_tibble$success <- as.numeric(full_functional_tibble$success)

full_functional_tibble
```

### Initial EDA

```{r echo=FALSE}
meta
```
This is the dataframe that contains some useful information about each mouse. It contains the date of the experiment (`date_exp`), the number of brain areas activated (`num_brain_area`), number of neurons activated (`num_neurons`), number of trials conducted that day on said mouse (`num_trials`) and the success rate of the mouse that day (`success_rate`). This dataframe will be used for visualization with the next 4 plots.


```{r echo=FALSE}
meta_subset <- meta %>%
  group_by(mouse_name) %>%
  summarize("brain_area_avg" = mean(num_brain_area))
ggplot(data = meta_subset, aes(x = mouse_name, y = brain_area_avg)) + 
  geom_col() +
  labs(
    title = "Average number of activated brain areas for each mouse",
    x = "Mouse Name",
    y = "Average Number of activated brain areas"
  )
```

```{r echo=FALSE}
meta_subset <- meta %>%
  group_by(mouse_name) %>%
  summarize("num_neurons_avg" = mean(num_neurons))
ggplot(data = meta_subset, aes(x = mouse_name, y = num_neurons_avg)) + 
  geom_col() +
  labs(
    title = "Average neuron activation for each mouse",
    x = "Mouse Name",
    y = "Average Neuron Activation"
  )
```

```{r echo=FALSE}
meta_subset <- meta %>%
  group_by(mouse_name) %>%
  summarize("num_trials_avg" = mean(num_trials))
ggplot(data = meta_subset, aes(x = mouse_name, y = num_trials_avg)) + 
  geom_col() +
  labs(
    title = "Average number of Trials for each mouse",
    x = "Mouse Name",
    y = "Average Number of Trials"
  )
```


```{r echo=FALSE}
meta_subset <- meta %>%
  group_by(mouse_name) %>%
  summarize("success_rate_avg" = mean(success_rate))
ggplot(data = meta_subset, aes(x = mouse_name, y = success_rate_avg)) + 
  geom_col() +
  labs(
    title = "Average sucess rate for each mouse",
    x = "Mouse Name",
    y = "Average success rate"
  )

```

These are some simple plots to understand some metrics about the 4 mice that we will be examining. In the first plot, we are examining the average number of brain areas activated for each mouse. In this case we see that Hench has the largest amount. In the next chart, we are examining the average number of neurons activated for each mouse and this case Forssmann seems to have the highest neuron activation. This is not what I expected as Hench had the most brain area activation so we would think that Hench would also have the highest neuron activation but that was not the case. The next graph examines the average number of trials per mouse and we see that Hench has the highest number of trials. The last graph looks at the average success rate for each mouse. In this graph, the mouse with the highest success rate seems to be Lederberg. This is interesting because this mouse was not top in any of the other categories but seems to have the highest average success rate. 


Now we will look into specific neuron trends for a subset of trials.


### Neuron-Specific EDA

```{r include=FALSE}
colnames(trial.summary)
```

```{r echo=FALSE}
plot.trial<-function(i.t,area, area.col,this_session){
    
    spks=this_session$spks[[i.t]];
    n.neuron=dim(spks)[1]
    time.points=this_session$time[[i.t]]
    
    plot(0,0,xlim=c(min(time.points),max(time.points)),ylim=c(0,n.neuron+1),col='white', xlab='Time (s)',yaxt='n', ylab='Neuron', main=paste('Trial ',i.t, 'feedback', this_session$feedback_type[i.t] ),cex.lab=1.5)
    for(i in 1:n.neuron){
        i.a=which(area== this_session$brain_area[i]);
        col.this=area.col[i.a]
        
        ids.spike=which(spks[i,]>0) # find out when there are spikes 
        if( length(ids.spike)>0 ){
            points(x=time.points[ids.spike],y=rep(i, length(ids.spike) ),pch='.',cex=4, col=col.this)
        }
      
            
    }
    
legend("topright", 
  legend = area, 
  col = area.col, 
  pch = 16, 
  cex = 0.8
  )
}

varname=names(trial.summary);
area=varname[1:(length(varname)-4)]
```

```{r echo=FALSE}
area.col=rainbow(n=n.area,alpha=0.7)
varname=names(trial.summary);
area=varname[1:(length(varname)-4)]
par(mfrow=c(1,1))
plot.trial(1,area, area.col,session[[i.s]])
mtext("Session 5 Trial 1 feedback", side = 1, line = 4.1, font = 1)
```
```{r echo=FALSE}
plot.trial(5,area, area.col,session[[i.s]])
mtext("Session 5 Trial 5 feedback", side = 1, line = 4.1, font = 1)
```
```{r echo=FALSE}
plot.trial(10,area, area.col,session[[i.s]])
mtext("Figure 2.3 Session 5 Trial 10 feedback", side = 1, line = 4.1, font = 1)
```

```{r echo=FALSE}
plot.trial(15,area, area.col,session[[i.s]])
mtext("Figure 2.4 Session 5 Trial 15 feedback", side = 1, line = 4.1, font = 1)

```
```{r echo=FALSE}
plot.trial(20,area, area.col,session[[i.s]])
mtext("Figure 2.5 Session 5 Trial 20 feedback", side = 1, line = 4.1, font = 1)
```


All these charts represent different trials in Session 5 and Neuron Activation at different points in time. The colors here represent different parts of the visual cortex (ACA, CA1, DG, MOs, OLF, ORB, PL, root, SUB, VISa). If we take a quick look at the graph we can see that the predominant neurons that get activated are those in the PL, root and SUB region of the visual cortex.One of the main things that we can take away from these types of graphs is that as time goes on in each trial, more neurons start to be activated. This is probably due to the fact that more stimulus is perceived by the eye and therefore the visual cortex meaning that more neurons are starting to be activated. 


Next we will take a step back and look at the spike counts over the whole session.


### Average Neuron Spike Rate EDA

```{r echo=FALSE}

# Convert trial.summary to long format
trial_summary_long <- trial.summary %>%
  pivot_longer(cols = -c(id, feedback, `left contr.`, `right contr.`), 
               names_to = "Area", values_to = "Spike_Count")

# Create the plot
ggplot(trial_summary_long, aes(x = id, y = Spike_Count, color = Area)) +
  geom_line() +
  geom_smooth(method = "loess", se = FALSE, size = 1) +
  labs(title = paste("Average Spike Count Across Trials in Session", i.s),
       x = "Trials", y = "Average Spike Counts") +
  theme_minimal() +
  theme(legend.position = "top") +
  guides(color = guide_legend(override.aes = list(linetype = 1)))
```

This chart shows us the average spike counts per trial in Session 5, colored based on the different regions of the visual cortex. I plotted two different lines on this graph. The more jagged line is the true average spike count per trial based on the region and I also plotted a smoothed version of the line without the noise which shows the general trend of the average spike count over the trials. One of things that I can quickly notice from this is that the root spike count decreases over the number of trials and actually approaches the second highest spike count region with is the DG. This might be an overall trend that the root spike decreases over time whereas the DG spikes go up as the mice learns. 


Next I will be trying to understand some homogeneity or heterogeneity across mice.


### Mouse Homogeneity/Heterogeneity EDA

```{r echo=FALSE}
meta$mouse_name <- as.factor(meta$mouse_name)

meta$num_new_per_area <- meta$num_neurons / meta$num_brain_area

ggplot(meta, aes(x = num_new_per_area, y = success_rate, group = mouse_name, color = mouse_name)) +
  geom_line() +
  geom_point() +
  labs(x = "Neurons per Area", y = "Success Rate", 
       title = "Success Rate vs. Density of Neurons per Area") +
  theme_minimal()

```


One of the theories that I postulated was that if there were more Neurons per Area that were activated, then the success rate would be higher. (increased brain activity, leading to increased cognition, which leads to better success?) That I why I wanted to create this graph. This graph shows the Success Rate vs. Neurons per Area which is divided by 4 mice. In one way, my theory is somewhat correct. All of the trials with the highest Neurons per Area for each mouse have the the highest success rate (except for Cori). This might suggest that Neurons per Area might be a good predictor for success and therefore the neural data might be useful for our prediction model.


```{r echo=FALSE}
meta$mouse_name <- as.factor(meta$mouse_name)


ggplot(meta, aes(x = num_trials, y = success_rate, group = mouse_name, color = mouse_name)) +
  geom_line() +
  geom_point() +
  labs(x = "Number of Trials", y = "Success Rate", 
       title = "Success Rate vs. Number of Trials Conducted") +
  theme_minimal()

```
Another theory that I thought about was that if a session contained more trials, then the success rate would be higher. This is because I thought given more trials, the mouse would be able to learn the behavior and therefore they would have a higher success rate. However, as can be seen from this graph, which plots Success Rate vs. Number of Trials, it seems like there is no real trend between the number of trials and success rate for each of the mice. The only one that increases is for the mouse "Forssmann", however, since it's only 1 occurence we can't make that assumption for any other mouse.

After all this EDA, I wanted to look at all of my data using scatter visualizations. To do this, I decided to use some dimensionality reduction techniques.

### Dimensionality Reduction

```{r echo=FALSE}
scaled <- full_functional_tibble[,1:40]
pca <- prcomp(scaled)
pca_df <- as.data.frame(pca$x)
pca_df$session_id <- full_functional_tibble$session_id
pca_df$mouse_name <- full_functional_tibble$mouse_name
head(pca_df)
```

The first dimensionality reduction technique I decided to use was Principal Component Analysis (PCA). PCA is a linear dimensionality reduction technique that projects points in a high dimensional space onto a line, called a principal component, while trying to maximize variance and reduce information loss. Above is a data frame with a PC representation the data. Now we will try to visualize the top 2 PCs based on some categorical variable.



```{r echo=FALSE}
ggplot(data = pca_df, aes(x = PC1, y = PC2, color = session_id )) +
  geom_point()+
  labs(
    title = "Principal Component Analysis: PC1 vs PC2 by Session ID"
  )
```

This a 2-d representation of our data. On the x-axis, I graphed the 1st principal component and on the y-axis, I graphed the 2nd principal component. I also labeled the points by the session id to see if there were any correlation between the session id and the principal components. It seems that each of the sessions have points that are clustered similarly however, it seems like there isn't much correlation between the session and the PCs. Let's try to see if they are correlated to the mice themselves.

```{r echo=FALSE}
ggplot(data = pca_df, aes(x = PC1, y = PC2, color = mouse_name )) +
  geom_point()+
  labs(
    title = "Principal Component Analysis: PC1 vs PC2 by Mouse Name"
  )
```
This plot is the same one as the one above, however, the coloring of points is based on the mouse rather than the session id. In this case I can't really notice anything that stands out to me as most of the mice's points overlap with each other which means that there might not be any correlation between the PCs and the mice. I then hypothesized that this might have something to do with the PCA in general. That is why I created the plot below.



```{r echo=FALSE}
plot(pca, type = 'l')
```
This a plot with the number of PCs on the x-axis and the explained variance on the y-axis. This plot shows a sharp decline in explained variance from PC1 to PC2. What this means is that the data might have a very complex structure which might not be able to be captured by the linear combinations of PCA. This is why I decided to try a different dimensionality reduction technique called t-SNE.

```{r include=FALSE}
library(Rtsne)
scaled <- full_functional_tibble[,1:40]
tsne_result <- Rtsne(scaled)
```


```{r echo=FALSE}
tsne_df <- as.data.frame(tsne_result$Y)
tsne_df$session_id <- full_functional_tibble$session_id
tsne_df$mouse_name <- full_functional_tibble$mouse_name
head(tsne_df)
```

Above is a data-frame from doing t-SNE. In this case V1 and V2 represent the two components that are created. t-SNE stands for t-Distributed Stochastic Neighbor Embedding. This is a dimensionality reduction technique, like PCA, but it "pairs high dimensional data points as conditional probabilities. It then minimizes the differences in the conditional probabilities in the higher dimensions and the probabilities in the lower dimensional space". One of the benefits of t-SNE is that it is a non-linear technique so it might be useful as our dataset it too complex for PCA use. Let's examine some of the plots.


```{r echo=FALSE}
ggplot(data = tsne_df, aes(x = V1, y = V2, color = session_id )) +
  geom_point()+
  labs(
    title = "t-SNE: V1 vs V2 by Session ID"
  )
```
This is the first plot that I created with t-SNE. On the x-axis we have the V1 component and on the y-axis we have the V2 component. The color is also determined by the `session_id`. In this case, it doesn't look like there is any trend between `session_id` and the two components. However, this does seem like a better visualization of our data.

```{r echo=FALSE}
ggplot(data = tsne_df, aes(x = V1, y = V2, color = mouse_name )) +
  geom_point()+
  labs(
    title = "t-SNE: V1 vs V2 by Mouse Name"
  )
```
Now looking at the plot above, it is the same as the one created before however I have labeled the points by the `mouse_name`. This plot doesn't seem to show any trends but it shows where some of the points lie with respect to the `mouse_name`.


Next I wanted to try some clustering analysis to see if I could gain anything from that.


### Cluster analysis:

```{r echo=FALSE}
tsne_cluster <- as.data.frame(tsne_result$Y)
kcluster <- kmeans(tsne_cluster[, c("V1", "V2")], centers = 4)

# Add cluster information to tsne_cluster data frame
tsne_cluster$cluster <- as.factor(kcluster$cluster)

# Plot the t-SNE results with data points colored by cluster
ggplot(data = tsne_cluster, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering: V1 vs V2")
```

In this example I tried to do some cluster analysis. Specifically I tried to use k-means clustering on the data. Since our data is of very high dimension, we could not visualize our data as is. Therefore I used my t-SNE version of my data to cluster. As you can see above the clustering doesn't seem to be great (it seems like lines are just drawn at certain points) so I would say that clustering hasn't given us any real value.

Now after all this EDA, we need can now begin our modeling process. However, before beginning our modeling process, we need to get our data into the the proper format. Now I will go over my data integration process.


# Part 3: Data Integration

In this section our goal is to transform the data into the form that we want to then use for our predictive modeling section. Below is some of the code that I used to do that.

```{r include = TRUE}
binename <- paste0("bin", as.character(1:40))

get_trial_functional_data <- function(session_id, trial_id){
  spikes <- session[[session_id]]$spks[[trial_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trial_bin_average <- matrix(colMeans(spikes), nrow = 1)
  colnames(trial_bin_average) <- binename
  trial_tibble  = as_tibble(trial_bin_average)%>% add_column("trial_id" = trial_id) %>% add_column("contrast_left"= session[[session_id]]$contrast_left[trial_id]) %>% add_column("contrast_right"= session[[session_id]]$contrast_right[trial_id]) %>% add_column("feedback_type"= session[[session_id]]$feedback_type[trial_id])
  
  trial_tibble
}
get_session_functional_data <- function(session_id){
  n_trial <- length(session[[session_id]]$spks)
  trial_list <- list()
  for (trial_id in 1:n_trial){
    trial_tibble <- get_trial_functional_data(session_id,trial_id)
    trial_list[[trial_id]] <- trial_tibble
  }
  session_tibble <- as_tibble(do.call(rbind, trial_list))
  session_tibble <- session_tibble %>% add_column("mouse_name" = session[[session_id]]$mouse_name) %>% add_column("date_exp" = session[[session_id]]$date_exp) %>% add_column("session_id" = session_id) 
  session_tibble
}

```

These are the functions I used to extract the data from the original data and properly format it. The first thing I did was I created a function to get the data from each trial. I first indexed the spikes and then I checked for any missing values. Then I calculated the average by using the `colMeans()` function. Then I assembled the rest of the table by adding the `trial_id`, `contrast_left`, `contrast_right`, and the `feedback_type` variables. The next function below gets all the important data from the session. First we want this function to call gather information for each trial so we instantiate a for loop for it to call the `get_trial_functional_data` function to gather information for all the trials in the session. I then make sure to add all relevant columns to the dataframe including the `mouse_name`, the `date_exp`, and the `session_id`. However, these are just the 2 functions. How did I make the full dataframe? That is done by the code-snippet below.

```{r, include= TRUE}
session_list = list()
for (session_id in 1: 18){
  session_list[[session_id]] <- get_session_functional_data(session_id)
}
full_functional_tibble <- as_tibble(do.call(rbind, session_list))
full_functional_tibble$session_id <- as.factor(full_functional_tibble$session_id )
full_functional_tibble$contrast_diff <- abs(full_functional_tibble$contrast_left-full_functional_tibble$contrast_right)

full_functional_tibble$success <- full_functional_tibble$feedback_type == 1
full_functional_tibble$success <- as.numeric(full_functional_tibble$success)

head(full_functional_tibble)
```
In this code chunk you can see we go through all 18 sessions and call our `get_session_functional_data` function which in turn calls our `get_trial_functional_data` function. Once we get all that data, make sure our session_id is actually a categorical factor by using the `as.factor()` method. I then also added an additional column which took into account the difference in contrast between contrast left and right. Then I also added a column called success as binary variable that is 0 when the mouse is unsuccessful and 1 when it is. Above you can see how the `full_functional_tibble` looks like. This is what I will be using for my modeling.


```{r include=TRUE}
set.seed(15)
sample <- sample(c(TRUE, FALSE), nrow(full_functional_tibble), replace=TRUE, prob=c(0.8,0.2))
train <- full_functional_tibble[sample, ]
test<- full_functional_tibble[!sample, ]
```

Next I wanted to prepare my data for model training and testing. Therefore I split my data into a training and testing set with 80% being in the training and 20% being in the testing. NOTE: I will be using another testing set, so think of this testing as more of a validation set. 

```{r include=TRUE}
train_x <- train %>%
  select(-c("mouse_name", "feedback_type", "date_exp", "session_id", "success"))
train_y <- train %>%
  select("feedback_type")
test_x <- test %>%
  select(-c("mouse_name", "feedback_type", "date_exp", "session_id", "success"))
test_y <- test %>%
  select("feedback_type")
  
```

For some models, I needed to split the features and the labels. That is why I used the `train_x`, `train_y`, `test_x` and `test_y` dataframes. In the data-matrices, I removed the categorical features and the `success` and `feedback_type` variables (these were going to be my labels). In the label matrices I just had the `feedback_type` as this was what I was going to predict.


Now that we have all of our data in order, we can finally start on our predictive modeling!


# Part 4: Predictive Modeling

In this section, I will be building 4 different models and assessing their viability by using some performance metrics. The first model that we will be using is a Logistic Regression model. This model is actually a binary classification model (unlike its name suggests) and is probably the simplest model that we can use to predict the `feedback_type` of these mice.

```{r echo=FALSE}
df <- train %>%
  select(-c("mouse_name", "date_exp", "success", "session_id"))
df$feedback_type[df$feedback_type<0] <- 0
model <- glm(feedback_type ~., data = df, family = "binomial")
summary(model)

df_test <- test %>%
  select(-c("mouse_name", "success", "date_exp", "session_id"))
predicted <- predict(model, newdata = df_test, type = "response")
predicted_class <- ifelse(predicted > 0.5, 1, -1)

accuracy <- mean(predicted_class == test_y)
print(paste("Accuracy:", accuracy))

conf_matrix <- table(df_test$feedback_type, predicted_class)

```
In this case we can see our model fit with all the predictors above. The stars in this case represent the most significant features to the prediction of `feedback_type`. Our accuracy for this model seems to be around 71%. For a very simple model like a logistic regression model, this seems pretty good. Let's take a look at the confusion matrix for this model and see where it was not performing so well.

```{r echo=FALSE}
conf_matrix_df <- as.data.frame.table(conf_matrix)
conf_matrix_df

conf_matrix_df$Freq <- as.numeric(conf_matrix_df$Freq)

# Plot the confusion matrix using ggplot
ggplot(data = conf_matrix_df, aes(x = Var1, y = predicted_class, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(x = "Predicted Class", y = "Actual Class", title = "Confusion Matrix") +
  theme_minimal()
```
This confusion matrix shows us the how many values were missclassified. In this case we can see that the model did pretty well at classifying positive values, however, it did not perform well when classifying negative or failure values. This is an interesting observation as maybe negative feedback might be more difficult to predict as maybe it's are too similar to the positive feedback. Let's see if we can improve on that.


```{r echo=FALSE}
library(randomForest)
df$feedback_type <- as.factor(df$feedback_type)
# Train the Random Forest classifier
rf_model <- randomForest(feedback_type ~ ., data = df)
```

```{r echo=FALSE}
# Predict labels on the test set
predictions <- predict(rf_model, df_test)
predicted_class <- ifelse(predicted > 0.5, 1, -1)

# Calculate accuracy
accuracy <- mean(predicted_class == df_test$feedback_type)
print(paste("Accuracy:", accuracy))

# Get feature importances
importance <- importance(rf_model)
# Plot feature importances
varImpPlot(rf_model)

```
The next model that I tried was called a Random Forest model. This is a tree based algorithm that creates n trees and employs an ensemble model method called bagging to help improve accuracy. Bagging works by creating a lot of weak learners (decision trees) and they train on different subsets of the dataset. They then spit out some sort of response, and the final response is calculated by some method like majority voting (i.e. pick the most common answer). In this case our Random Forest model gave us the same accuracy as our logistic regression model at around 71%. One of the things we can also use the Random Forest model for is to determine feature importances. This can help us understand which features contribute most to the prediction. In this case based on the diagram above, we see `trial_id` and `contrast_diff` seem to have the highest importance. In terms of model interpretation, we might be thinking that these two features are very correlated with `feedback_type`, however, we would need to conduct additional analysis to be sure of that.

Next I will be trying to use another ensemble tree method that is popular with this data: XGBoost.



```{r echo=FALSE}
library(xgboost)


# Convert data frame to matrix
train_matrix <- as.matrix(train_x)
train_matrix_labels <- as.matrix(train_y)

train_y_binary <- ifelse(train_y == -1, 0, 1)
train_y_binary <- as.matrix(train_y_binary)

# Train the XGBoost model
xgb_model <- xgboost(data = train_matrix, label = train_y_binary, objective = "binary:logistic", nrounds = 10)
```

```{r echo=FALSE}
test_x <- as.matrix(test_x)
test_y <- as.matrix(test_y)
predictions <- predict(xgb_model, newdata = test_x)
predicted_labels <- as.numeric(ifelse(predictions > 0.5, 1, -1))
accuracy <- mean(predicted_labels == test_y)
print(paste("Accuracy:", accuracy))
```

The next model that I used was called XGBoost. This is a very popular gradient boosting algorithm (which is another type of ensembling method) and works particularly well with tabular data. If we look at the accuracy, we achieved a significantly higher accuracy of around 73%. Seems like XGBoost does work well after all! Next I wanted to look at why XGBoost was particularly good on our data. I constructed a Confusion matrix to look at the values.


```{r echo=FALSE}
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}

# Load the caret package
library(caret)
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(test_y))
confusion_matrix <- conf_matrix$table

row_names <- rownames(confusion_matrix)
col_names <- colnames(confusion_matrix)

conf_df <- data.frame(
  Reference = rep(row_names, each = length(col_names)),
  Prediction = rep(col_names, length(row_names)),
  Freq = as.vector(confusion_matrix)
)

# Convert the data types to factor for correct plotting
conf_df$Reference <- factor(conf_df$Reference, levels = row_names)
conf_df$Prediction <- factor(conf_df$Prediction, levels = col_names)

# Plot the confusion matrix using ggplot2
ggplot(data = conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Confusion Matrix",
       x = "True Labels",
       y = "Prediction") +
  theme_minimal()
```
Here is the confusion matrix for our XGBoost model. It seems like something that it did better than our Logistic Regression model was classifying the negative feedback. Even though it didn't do the best job, it is nice to see that there was an improvement with our model.


In the last model, I want to try a model that might be good at classifying this high dimensional data: a SVM.


```{r echo=FALSE}
library(e1071)
df <- train %>%
  select(-c("mouse_name", "date_exp", "success", "session_id"))
svm_model <- svm(feedback_type ~ ., data = df, kernel = "radial")

df_test <- test %>%
  select(-c("mouse_name", "success", "date_exp", "session_id"))
predictions <- predict(svm_model, df_test)
predicted_labels <- as.numeric(ifelse(predictions > 0.5, 1, -1))
# Evaluate the performance of the SVM classifier
accuracy <- mean(predicted_labels == test_y)

print(paste("Accuracy:", accuracy))
```
The last model that I used was a Support Vector Machine (SVM). Support vector machines use a kernel function to help understand and classify high-dimensional data. Since our data fit those categories, I believed that a SVM would be a good fit. As you can see here, our accuracy was pretty good at around 72%. While this certainly was not as good as our XGBoost model, I think it performed pretty well on the data and it was definitely better than our Random Forest and Logistic Regression models.

Since the SVM tries to create a decision boundary on high dimensional data. I wanted to visualize its results on the testing dataset. Since we can't see all 40+ dimensions of the dataset, I decided to use a dimensionality reduction technique to create the graph below.


```{r echo=FALSE}
library(ggplot2)
library(e1071)

# Perform PCA on the test data
pca <- prcomp(test_x)
pca_subset <- pca$x[, 1:2]

# Predict labels using the SVM model
predictions <- predict(svm_model, test_x)
predicted_labels <- as.numeric(ifelse(predictions > 0.5, 1, -1))

# Create a data frame for plotting
plot_data <- data.frame(pca_subset, Predicted_Label = predicted_labels)
colnames(plot_data) <- c("PC1", "PC2", "Predicted_Label")

# Convert Predicted_Label to factor
plot_data$Predicted_Label <- as.factor(plot_data$Predicted_Label)

# Plot the PCA results with data points colored by predicted labels
ggplot(plot_data, aes(x = PC1, y = PC2, color = Predicted_Label)) +
  geom_point() +
  scale_color_manual(values = c("blue", "red")) +
  labs(color = "Predicted Label") +
  theme_minimal()


```
The plot above shows the classification of the test data. I visualized this data by using PCA to bring down the dimensions of the data to 2 and plotted the classification (either -1 or 1). Some trends that I noticed were that all the data points seem to lie on these 4 lines/bands which might explain some homogeneity within the test set. In addition, it seemed like most of the predicted negative feedback points had a PC1 value of around 100 or greater. In addition we see that in the test set, there are alot more values for positive feedback than negative feedback which might be of concern for us.


After all this model building and evaluation, I will choose the XGBoost model as it performed the best on the validation split. Next we will evaluate this model on the test data.

# Part 5: Evalutation of Model on Test Set

```{r}
testing=list()
for(i in 1:2){
  file_path <- paste('/Users/tejgaonkar/Downloads/test/test', i, '.rds', sep='')
  testing[[i]] <- readRDS(file_path)
  print(testing[[i]]$mouse_name)
  print(testing[[i]]$date_exp)
}
```

```{r}
testing_list = list()
for (testing_id in 1:2){
  testing_list[[testing_id]] <- get_session_functional_data(testing_id)
}
full_functional_tibble_test <- as_tibble(do.call(rbind, testing_list))
full_functional_tibble_test$session_id <- as.factor(full_functional_tibble_test$session_id )
full_functional_tibble_test$contrast_diff <- abs(full_functional_tibble_test$contrast_left-full_functional_tibble_test$contrast_right)

full_functional_tibble_test$success <- full_functional_tibble_test$feedback_type == 1
full_functional_tibble_test$success <- as.numeric(full_functional_tibble_test$success)

head(full_functional_tibble_test)
```

```{r}
testing_x <- full_functional_tibble_test %>%
  select(-c("mouse_name", "date_exp", "session_id", "success", "feedback_type"))
testing_y <- full_functional_tibble_test %>%
  select("feedback_type")
```

To load in the testing data I used the same process as before by loading all the .RDS files and passing them through the same functions that I had defined in the data integration part of the report. This produced the tibble that is above the preceding code-chunk. For my XGBoost model, I need to divide the data into a testing data matrix with all the relevant features and a testing label matrix with all the corresponding labels. These are the two subsets of the dataframe that I created above.

Next I moved on to evaluating the model.

```{r echo = FALSE}
testing_x <- as.matrix(testing_x)
testing_y <- as.matrix(testing_y)
predictions <- predict(xgb_model, newdata = testing_x)
predicted_labels <- as.numeric(ifelse(predictions > 0.5, 1, -1))
accuracy <- mean(predicted_labels == testing_y)
print(paste("Accuracy:", accuracy))
```
The first evaluation metric I used was accuracy and in this case the model performed pretty well with around 78% accuracy. This is better than the 73% accuracy on the validation set however, it could be an outlier due to the nature of the data. Regardless it seems like it performed well on the testing data.

Next I wanted to make an ROC and calculate the AUC to evaluate model performance

```{r echo = FALSE}
# Make predictions on the test data
predicted_probs <- predictions
# Calculate ROC curve and AUC
library(pROC)

suppressWarnings(roc_curve <- roc(testing_y, predicted_probs))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for XGBoost Model",
     xlab = "False Positive Rate", ylab = "True Positive Rate")


# Calculate AUC
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
```
In this case, I wanted to look at an ROC curve (Receiver Operating Characteristic Curve) and the AUC metric (Area Under ROC Curve). The ROC curve shows the false positive rate on the x-axis and the true-positive rate on the y-axis. The diagonal line represents a random classifier, which is just classifying the two classes randomly. A perfect classifier would be having a line going towards the top left corner. A easier metric to use is the AUC which measures the Area underneath the ROC curve. If your AUC is 0.5, then your classifier is essentially random and 1 indicates the perfect classifier. An area between 0.5 and 1 tells you the effectiveness of your classifier with values closer to 1 being desired. Our AUC value for the test set was 0.86 which is very good. AUC is a good metric to use in this case because our testing data is slightly imbalanced. AUC works well on imbalanced data and gives us a good metric on how good our data is to use.

Like always, we want to see how our model classified different categories. Therefore, I made a confusion matrix below.


```{r echo=FALSE}
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}

# Load the caret package
library(caret)
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(testing_y))
confusion_matrix <- conf_matrix$table

row_names <- rownames(confusion_matrix)
col_names <- colnames(confusion_matrix)

conf_df <- data.frame(
  Reference = rep(row_names, each = length(col_names)),
  Prediction = rep(col_names, length(row_names)),
  Freq = as.vector(confusion_matrix)
)

# Convert the data types to factor for correct plotting
conf_df$Reference <- factor(conf_df$Reference, levels = row_names)
conf_df$Prediction <- factor(conf_df$Prediction, levels = col_names)

# Plot the confusion matrix using ggplot2
ggplot(data = conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Confusion Matrix",
       x = "True Labels",
       y = "Prediction") +
  theme_minimal()
```
This confusion matrix here shows the classification using XGBoost on the testing data. It does a much better job at classifying the negative feedback and the positive feedback. However, this might be due to data imbalance as there were more positive feedback values (228) than negative feedback values (137). This is why I think that AUC is a much better metric in this case, however, it is good to visualize the true performance of our model using a confusion matrix.

That concludes the testing section of the report. We will move on now to our final section: Discussion


# Part 6: Discussion

In this report, we tried to come up with the best predictor to predict feedback types on mice based on their neural activities. I first started out with some Exploratory Data Analysis, trying to understand the underlying trends and features in the data by looking at the meta-data, the specific neural data, the spikes across sessions, understand different trends between mice, and finally some dimensionality reduction and clustering analysis. I then explained how I was able to integrate my data using functions and averaging out the spike counts to feed into a predictor. I also explained how I subsetted my data into training and testing (really validation) sets. Then I talked about the 4 different models I tried: Logistic Regression, Random Forest Classification, XGBoost, and SVM and I talked about their model accuracies and other metrics I used to evaluate their performance. Finally, I chose the XGBoost model as it had the best accuracy. Then I evaluated the XGBoost model on the test set by first integrating the test set using the data integration method that I described, and then evaluating it based on Accuracy, AUC (with an ROC curve) and a confusion matrix.

Throughout this project, the biggest issue was dealing with the unnatural nature of the data. First of all the data could not be integrated into a dataframe/tibble without using some sort of aggregate function. Even with the approach that I took, I definitely lost some depth of information with the data. This means that there might be other approaches that keep the data more intact and therefore might have better performance. In addition, I noticed that all 3 subsets of the original data seemed to have more examples of positive feedback than negative feedback. Whenever I looked at a confusion matrix, I noticed that the model was pretty good at classifying positive feedback but very bad at negative feedback. If we had more instances or almost equal instances of both, our models might have been more accurate as they would truly be able to understand the relationship between the neural activities and the `feedback_type`.

If I were to take this project further, another thing that I would like to try is to use a deep learning model (i.e. a feed forward Neural Network). I initially tried using one however, for some reason it would take a very long time to run so I dropped it at the end. However, I think it would be interesting to see how a neural network would perform on this data and if it would give us good accuracy. Since all the data is already numerical, there would be little to no encoding needed for categorical features so I think in theory, a neural network should be pretty easy to implement, barring algorithmic and computation constraints.

Overall, throughout my report, I found that the XGBoost model was the best at predicting `feedback_type` given neural activities, `trial_id`, `contrast_left`, `contrast_right`, and `contrast_diff`. Our model achieved 73% accuracy on the validation set and 78% accuracy on the testing set with a 0.86 AUC score. This was initially what I thought of because XGBoost has been a notoriously good learning algorithm on tabular data and has been very reliable for users in the past. Although this task is not as easy as it might first seem, XGBoost does a good job at modeling this data and coming up with a reliable prediction.


```{r include=FALSE}
sessionInfo()
```

# References {-}
1. ChatGPT (For ideas and questions about specific libraries)
2. Lecture Notes
3. Discussion Notes
4. Other Students (To understand requirements and details for assignments)






